{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Yesterday the weather was sunny and windy. Today it is sunny and windy too.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yesterday',\n",
       " 'the',\n",
       " 'weather',\n",
       " 'was',\n",
       " 'sunny',\n",
       " 'and',\n",
       " 'windy',\n",
       " 'today',\n",
       " 'it',\n",
       " 'is',\n",
       " 'sunny',\n",
       " 'and',\n",
       " 'windy',\n",
       " 'too']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text: str) -> list:\n",
    "    import re\n",
    "    try:\n",
    "        return re.findall('\\w+', text.lower())\n",
    "    except AttributeError:\n",
    "        return []\n",
    "    \"\"\"\n",
    "    Splits sentences into tokens, converts the tokens into lowercase, removes punctuation\n",
    "    :param text: the initial text\n",
    "    :return: a list of lowercased tokens without punctuation\n",
    "    e.g. text = 'The weather is sunny, the man is happy.'\n",
    "    --> ['the', 'weather', 'is', 'sunny', 'the', 'man', 'is', 'happy']\n",
    "    \"\"\"\n",
    "\n",
    "tok_text = tokenize(text)\n",
    "tok_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yesterday', 'weather', 'sunny', 'windy', 'today', 'sunny', 'windy']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stop_words(tokens: list, stop_words: list) -> list:\n",
    "    if type(tokens) is list and all(type(s) is str for s in tokens):    # check tokens\n",
    "        if type(stop_words) is list and all(type(s) is str for s in stop_words):    # check stop-words\n",
    "            return [word for word in tokens if word not in stop_words]\n",
    "        else:\n",
    "            return tokens\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "    \"\"\"\n",
    "    Removes stop words\n",
    "    :param tokens: a list of tokens\n",
    "    :param stop_words: a list of stop words\n",
    "    :return: a list of tokens without stop words\n",
    "    e.g. tokens = ['the', 'weather', 'is', 'sunny', 'the', 'man', 'is', 'happy']\n",
    "    stop_words = ['the', 'is']\n",
    "    --> ['weather', 'sunny', 'man', 'happy']\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "stop_words = open('stop_words.txt').read().split()\n",
    "clean_text = remove_stop_words(tok_text, stop_words)\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'windy': 2, 'sunny': 2, 'weather': 1, 'yesterday': 1, 'today': 1}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_frequencies(tokens: list) -> dict:\n",
    "    d = {}\n",
    "    if type(tokens) is list and all(type(s) is str for s in tokens):    # check tokens\n",
    "        for word in set(tokens):\n",
    "            d[word] = tokens.count(word)\n",
    "        d = dict(sorted(d.items(), key=lambda x: x[1], reverse=True))\n",
    "    return d\n",
    "        \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates frequencies of given tokens\n",
    "    :param tokens: a list of tokens without stop words\n",
    "    :return: a dictionary with frequencies\n",
    "    e.g. tokens = ['weather', 'sunny', 'man', 'happy']\n",
    "    --> {'weather': 1, 'sunny': 1, 'man': 1, 'happy': 1}\n",
    "    \"\"\"\n",
    "\n",
    "freq_dict = calculate_frequencies(clean_text)\n",
    "freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sunny', 'windy', 'today']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_top_n_words(freq_dict: dict, top_n: int) -> list:\n",
    "    if type(freq_dict) is dict and all(type(s) is str for s in freq_dict) and type(top_n) is int:    # check freq_dict\n",
    "        return list(freq_dict.keys())[:top_n]\n",
    "    return []\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the most common words\n",
    "    :param freq_dict: a dictionary with frequencies\n",
    "    :param top_n: a number of the most common words to return\n",
    "    :return: a list of the most common words\n",
    "    e.g. tokens = ['weather', 'sunny', 'man', 'happy', 'and', 'dog', 'happy']\n",
    "    top_n = 1\n",
    "    --> ['happy']\n",
    "    \"\"\"\n",
    "\n",
    "top = get_top_n_words(freq_dict, 3)\n",
    "top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'weather', 'was', 'sunny', 'and'],\n",
       " ['today', 'it', 'is', 'sunny', 'and']]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_concordance(tokens: list, word: str, left_context_size: int, right_context_size: int) -> list:\n",
    "    if type(tokens) is list and all(type(s) is str for s in tokens) and word in tokens:    # check tokens and word\n",
    "        if left_context_size >= 1 or right_context_size >= 1:\n",
    "            idx = [i for i, x in enumerate(tokens) if x == word]\n",
    "            conc = []\n",
    "            for i in idx:\n",
    "                conc.append(tokens[i-left_context_size:i+right_context_size+1])\n",
    "            return conc\n",
    "    return []\n",
    "    \n",
    "    \"\"\"\n",
    "    Gets a concordance of a word\n",
    "    A concordance is a listing of each occurrence of a word in a text,\n",
    "    presented with the words surrounding it\n",
    "    :param tokens: a list of tokens\n",
    "    :param word: a word-base for a concordance\n",
    "    :param left_context_size: the number of words in the left context\n",
    "    :param right_context_size: the number of words in the right context\n",
    "    :return: a concordance\n",
    "    e.g. tokens = ['the', 'weather', 'is', 'sunny', 'the', 'man', 'is', 'happy',\n",
    "                    'the', 'dog', 'is', 'happy', 'but', 'the', 'cat', 'is', 'sad']\n",
    "    word = 'happy'\n",
    "    left_context_size = 2\n",
    "    right_context_size = 3\n",
    "    --> [['man', 'is', 'happy', 'the', 'dog', 'is'], ['dog', 'is', 'happy', 'but', 'the', 'cat']]\n",
    "    \"\"\"\n",
    "conc = get_concordance(tok_text, 'sunny', 3, 1)\n",
    "conc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'and'], ['today', 'and']]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_adjacent_words(tokens: list, word: str, left_n: int, right_n: int) -> list:\n",
    "    conc = get_concordance(tokens, word, left_n, right_n)\n",
    "    if left_n == 0:\n",
    "        return [[elem[-1]] for elem in conc]\n",
    "    elif right_n == 0:\n",
    "        return [[elem[0]] for elem in conc]\n",
    "    else:\n",
    "        return [[elem[0], elem[-1]] for elem in conc]\n",
    "    \n",
    "    \"\"\"\n",
    "    Gets adjacent words from the left and right context\n",
    "    :param tokens: a list of tokens\n",
    "    :param word: a word-base for the search\n",
    "    :param left_n: the distance between a word and an adjacent one in the left context\n",
    "    :param right_n: the distance between a word and an adjacent one in the right context\n",
    "    :return: a list of adjacent words\n",
    "    e.g. tokens = ['the', 'weather', 'is', 'sunny', 'the', 'man', 'is', 'happy',\n",
    "                    'the', 'dog', 'is', 'happy', 'but', 'the', 'cat', 'is', 'sad']\n",
    "    word = 'happy'\n",
    "    left_n = 2\n",
    "    right_n = 3\n",
    "    --> [['man', 'is'], ['dog, 'cat']]\n",
    "    \"\"\"\n",
    "\n",
    "adj = get_adjacent_words(tok_text, 'sunny', 3, 1)\n",
    "adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_file(path_to_file: str) -> str:\n",
    "    \"\"\"\n",
    "    Opens the file and reads its content\n",
    "    :return: the initial text in string format\n",
    "    \"\"\"\n",
    "    with open(path_to_file, 'r', encoding='utf-8') as fs:\n",
    "        data = fs.read()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def write_to_file(path_to_file: str, content: list):\n",
    "    import os\n",
    "    with open(os.path.join(path_to_file, 'report.txt'), 'w', encoding='utf-8') as fs:\n",
    "        fs.write('\\n'.join([' '.join(k) for k in content]))\n",
    "\n",
    "write_to_file('', conc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_concordance(tokens: list, word: str, left_context_size: int, right_context_size: int, left_sort: bool) -> list:\n",
    "    if type(left_sort) == bool:\n",
    "        conc = get_concordance(tokens, word, left_context_size, right_context_size)\n",
    "        if left_sort:\n",
    "            return sorted(conc, key=lambda x: x[0])\n",
    "        print(conc[0][-right_context_size])\n",
    "        return sorted(conc, key=lambda x: x[-right_context_size])\n",
    "    return []\n",
    "    \"\"\"\n",
    "    Gets a concordance of a word and sorts it by either left or right context\n",
    "    :param tokens: a list of tokens\n",
    "    :param word: a word-base for a concordance\n",
    "    :param left_context_size: the number of words in the left context\n",
    "    :param right_context_size: the number of words in the right context\n",
    "    :param left_sort: if True, sort by the left context, False â€“ by the right context\n",
    "    :return: a concordance\n",
    "    e.g. tokens = ['the', 'weather', 'is', 'sunny', 'the', 'man', 'is', 'happy',\n",
    "                    'the', 'dog', 'is', 'happy', 'but', 'the', 'cat', 'is', 'sad']\n",
    "    word = 'happy'\n",
    "    left_context_size = 2\n",
    "    right_context_size = 3\n",
    "    left_sort = True\n",
    "    --> [['dog', 'is', 'happy', 'but', 'the', 'cat'], ['man', 'is', 'happy', 'the', 'dog', 'is']]\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['is', 'sunny', 'and', 'windy', 'too'],\n",
       " ['was', 'sunny', 'and', 'windy', 'today', 'it']]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_concordance(tok_text, 'windy', 3, 2, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['is', 'sunny', 'and'], ['was', 'sunny', 'and']]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conc = [['was', 'sunny', 'and'], ['is', 'sunny', 'and']]\n",
    "sorted(conc, key=lambda x: x[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
