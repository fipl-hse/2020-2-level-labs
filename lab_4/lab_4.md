# Лабораторная работа №4

## Дано

1. Текст на английском языке
2. Необходимо построить модель генерации текста

## Идея
В рамках данной лабораторной работы, как и в случае с третьей, вы будете работать с N-граммами.

Определение: n-грамма – это последовательность из n элементов,
в настоящей работе – последовательность из n **слов**, а не букв, как в предыдущей лабораторной работе.

Пример: `Somebody once told me the secret`
   * Уни-грамы: `[Somebody] [once] [told] [me] [the] [secret]`
   * Би-грамы: `[Somebody once] [once told] [told me] [me the] [the secret]`

Таким образом N-грамма состоит из нескольких слов. Для каждого из слов в N-грамме мы можем говорить о его соседях - 
некотором **контексте** данного слова.
Так как можно посчитать, сколько раз то или иное слово встречается в 
определённой N-грамме - то есть как часто слово встречается в определённом **контексте** - можно говорить о вероятностном
распределении слова в N-граммах из текста.

С этой информацией перейдём к идее генерации текста на основе N-грамм. Она состоит в следующем:

Последнее слово в N-грамме можно предсказать по словам до него - **контексту** данного слова. 
То есть, вероятность появления слова зависит от слов, которые встречаются до него. Мы добавляем новое слово к 
имеющимся, берём уже новую N-грамму из последних слов - предсказываем по ним и так далее. 

Таким образом нам не нужно смотреть на предложение или даже текст целиком, чтобы предсказать следующее слово: необходимо только иметь 
представление о некотором количестве `(n-1)` слов до текущего.

> **Важно** Сейчас необходимо понять идею - вычисление вероятности и выбор слова будет более подробно описан далее.

Пример для модели с `n = 3`, то есть с три-граммами:

Контекст: `I am going to ___`

Задача: продолжить этот текст.

> **Важно**: мы не бёрём весь текст целиком, а только последние `n-1` слов.

Представим, что у нас есть заранее вычисленное частотное распределение слов по N-граммам:

```python
frequencies = {
    ('going', 'to', 'swim'): 4,
    ('going', 'to', 'walk'): 3,
    ('to', 'walk', 'and'): 5,
    ('to', 'swim', 'and'): 10,
    ('swim', 'and', 'play'): 20,
    ('swim', 'and', 'cry'): 2
}
```

Алгоритм генерации текста:
1. Из контекста берём последовательность из `n-1` - в нашем случае это последние два слова: `('going', 'to')`;
2. Находим все три-граммы, которые начинаются с `('going', 'to')` (у нас их две);
3. Смотрим, для какой из три-грамм частота больше:
 * `F('going', 'to', 'swim') > F('going', 'to', 'walk')` 
4. Берём последнее слово из три-граммы с наибольшей частотой появления и добавляем в выходной текст;
 * В нашем случае берём `swim` и добавляем к выходному тексту, получается следующее: `I am going to swim`.
5. Теперь повторяем шаги 1-4 для новой три-граммы `('to', 'swim', '___')`.

Пример конечного сгенерированного текста:
`I am going to swim and play`.

> Критерии остановки генерации текста могут быть различны - о них подробнее непосредственно в реализации.

## Что необходимо сделать

### Шаг 1. Токенизация текста

Функция принимает на вход текст и возвращает кортеж токенов.
В качестве разделителя используется завершающий пунктуационный знак, пробел и факт того, что следующее слово начинается с большой буквы.

Если на вход подается некорректное значение, поднимается ошибка `ValueError`.

К каждому предложению в конце также прибавляется специальный символ: `<END>`. 
Вставка этих символов обязательна для корректной работы алгоритма в дальнейшем.

Например, `text = 'I have a cat.\nHis name is Bruno'`
--> `('i', 'have', 'a', 'cat', '<END>' ,'his', 'name', 'is', 'bruno', '<END>')`

**Интерфейс:**
```python
def tokenize_by_sentence(text: str) -> tuple:
  pass
```

### Шаг 2. Создание хранилища соответствий слово-число

> **Важно** Данный класс очень похож на хранилище из предыдущей лабораторной работы, единственное отличие: здесь мы работаем со словами.

Каждому слову из заданного текста присваивается некоторый уникальный идентификатор id. 
Это требуется для того, чтобы работать не со строками напрямую, а с числами, которые их представляют.

Например, слову `word` ставим в соответствие некоторое уникальное число - 12345. Следующему слову 'to' - 12346 и так далее. 
Эти слова необходимо поместить в поле `storage` класса `WordStorage`, ключом хранилища является слово, а значением – её идентификатор. 
Слова, указанные выше, будут храниться следующим образом:

> Идентификатор можно назначить на своё усмотрение. Главное условие: одному слову соответствует один уникальный идентификатор.

```python
self.storage = {
    'word': 12345,
    'to': 123456
}
```

**Интерфейс**:
```python
class WordStorage:
    pass
```

#### Шаг 2.1. Добавление слова в хранилище

Функция принимает на вход слово и добавляет её в хранилище `storage`.

Если слово было успешно добавлено в хранилище, возвращается присвоенный идентификатор.
При этом если буква уже существовала в хранилище, идентификатор остается прежним.

Если на вход подается некорректное значение, поднимается ошибка `ValueError`.
Пустая строка считается **некорректным** значением.

**Интерфейс**:
```python
class WordStorage:
  ...
  def _put_word(self, word: str) -> int:
    pass
```

#### Шаг 2.2. Получение идентификатора слова

Функция принимает на вход слово и возвращает его идентификатор.

**Интерфейс**:
```python
class WordStorage:
  ...
  def get_id(self, word: str) -> int:
    pass
```

Если на вход подается неизвестное слово, поднимается ошибка `KeyError`.

Если на вход подается некорректное значение, поднимается ошибка `ValueError`.

#### Шаг 2.3. Получение слова по идентификатору

Функция принимает на вход идентификатор и возвращает слово с этим идентификатором.

**Интерфейс**:
```python
class WordStorage:
  ...
  def get_word(self, word_id: int) -> str:
    pass
```

Если на вход подается неизвестный идентификатор, поднимается ошибка `KeyError`.

Если на вход подается некорректное значение, поднимается ошибка `ValueError`.

#### Шаг 2.4. Заполнение хранилища словами из корпуса предложений

Функция заполняет хранилище `storage` класса `WordStorage` словами из кортежа токенов.

Например,
```python
text = (
    'i', 'have', 'a', 'cat', '<END>', 'his', 'name', 'is', 'bruno', '<END>'
)
```

Если добавление прошло успешно, ничего не возвращается.
Если на вход подается некорректное значение, поднимается ошибка `ValueError`.

**Дополнительные требования:**

1. В данной функции ОБЯЗАТЕЛЬНО использовать метод `_put_word` (см. Шаг 2.1).

**Интерфейс**:
```python
class WordStorage:
  ...
  def update(self, corpus: tuple):
    pass
```

### Шаг 3. Кодирование текста. Выполнение Шагов 1-3 соответствует 4 баллам

Функция кодирует текст, состоящий из предложений, используя заполненный экземпляр класса `WordStorage`,
и возвращает кортеж закодированных токенов.
Кодирование заключается в замене слов на соответствующие идентификаторы.

Например, 
```python
text = (
    'i', 'have', 'a', 'cat', '<END>','his', 'name', 'is', 'bruno', '<END>'
)
         
encoded_text = (
                  1, 2, 3, 4, 5, 6, 
                  7, 8, 9, 10, 6,
                 )
```

Если на вход подаются некорректные значения, поднимается ошибка `ValueError`.

**Интерфейс**:

```python
def encode_text(storage: WordStorage, text: tuple) -> tuple:
  pass
```

> **Важно** В данной функции ОБЯЗАТЕЛЬНО использовать метод `get_id` экземпляра класса `WordStorage` (см. Шаг 2.2).

### Шаг 4. Создание модели генерации текста

Идея генерации текста с помощью N-грамм состоит в следующем: последнее слово в N-грамме может быть найдено исходя из 
предшествующих ему слов. 

В начале данного описания была рассмотрена идея выбора следующего слова на основе частотного распределения N-грамм. 
Эта идея позволит нам создать модель генерации текста, которая внутри себя будет содержать информацию о языковой 
модели текста на основе N-грамм - `NGramTrie`.

Модели необходимо следующее:
 * Заполненный экземпляр класса `WordStorage`;
 * Обученная на тексте модель `NGramTrie`.

> **Важно** Модель `NGramTrie` уже реализована, она идёт отдельным модулем в папке `ngrams`. 
> Вы должны с ней работать: создавать экземпляры класса и тренировать их на тексте. 
> Полную документацию о том, как её использовать, можно найти [здесь](./ngrams/README.md).

**Интерфейс**:
```python
class NGramTextGenerator:
    pass
```

**Пример** создания экземпляра класса:
```python
n_gram_text_generator = NGramTextGenerator(storage, trie)
```

 * `storage` - это заполненный ранее экземпляр класс WordStorage. Он должен храниться в поле `_word_storage`.
 * `trie` - это заполненный экземпляр класса `NGramTrie`. Он должен храниться в поле `_n_gram_trie`.


#### Шаг 4.1. Генерация следующего слова

Для генерации слова нам необходимо: 
 * Контекст (последние `n-1` слов);
 * Частотное распределение N-грамм в языке.

Самым простым способом для генерации следующего слова на основе имеющихся данных будет следующий алгоритм:
1. Найти все N-граммы, которые начинаются с заданного контекста;
2. Выбрать из них ту, которая имеет наибольшую частоту встречаемости;
3. Взять последнее слово из найденной N-граммы.

**Интерфейс**:
```python
class NGramTextGenerator:
  ...
  def _generate_next_word(self, context: tuple) -> int:
      pass
```

Если приходят некорректные значения (или некорректный ввод `context`, или его неправильный размер), то поднимается ошибка `ValueError`.

Если данного контекса нет в словаре N-грамм, то возвращается наиболее частотное слово.

> **Важно** В этой функции ОБЯЗАТЕЛЬНО использовать поля `n_gram_frequencies`, `uni_grams` экземпляра класса `NGramTrie`.

**Пример**:

Есть модель с размером N-грамм равным 3, она уже обучена на некотором тексте, выделены N-граммы, вычислены их частоты. 
Дан контекст: `(0, 3)`. Необходимо по этому контексту предсказать следующее слово.

Находим все возможные N-граммы, которые начинаются с токенов из контекста, и их частоты:

```
(0, 3, 4): 10, 
(0, 3, 124): 20, 
(0, 3, 4568): 30
```

Выбираем из этих N-грамм ту, которая имеет наибольшую частоту - в данном случае `(0, 3, 4568)`. 
Токен с идентификатором `4568` и будет предсказанным словом.

#### Шаг 4.2. Генерация предложения

Так как мы разбивали текст на предложения и добавляли `<END>` для индикации окончания предложения, у нас есть N-граммы,
содержащие данный токен. Это позволяет нам выделить критерий, по которому генерация текста может останавливаться:
 * Если следующий сгенерированный токен = `<END>`, то происходит остановка генерации предложения.

**Интерфейс**:
```python
class NGramTextGenerator:
  ...
  def _generate_sentence(self, context: tuple) -> tuple:
      pass
```
 
> **Важно** В этой функции ОБЯЗАТЕЛЬНО использовать метод `_generate_next_word`.

Если в течение 20 раз не был сгенерирован токен `<END>`, то генерация останавливается, а на последнюю позицию предложения ставится
 идентификатор токена `<END>`.

Если приходят некорректные значения, то поднимается ошибка `ValueError`.

**Пример** сгенерированного предложения:

`(0, 3, 2, 1, 5, 126, 4)`, где `4` - идентификатор токена `<END>`.

#### Шаг 4.3. Генерация текста. Выполнение Шагов 1-4.3 соответствует 6 баллам

Теперь, когда есть способ генерации целого предложения, можно переходить к тексту.

Алгоритм генерации текста можно представить следующим образом:
1. По данному контексту происходит генерация предложения;
2. Новый контекст образуется из последних `n-1` токенов сгенерированного предложения (включая токен `<END>`);
3. Шаги 1-2 повторяются для нового контекста.

Критерием остановки генерации текста будет генерация необходимого количества предложений.

**Интерфейс**:
```python
class NGramTextGenerator:
  ...
  def generate_text(self, context: tuple, number_of_sentences: int) -> tuple:
      pass
```

В качестве возвращаемого значения будет кортеж из закодированных слов.

> **Важно** В этой функции ОБЯЗАТЕЛЬНО использовать метод `_generate_sentence`.

Если приходят некорректные значения, то поднимается ошибка `ValueError`.

**Пример**:

Есть модель с размером N-грамм равным 3, она уже обучена на некотором тексте, выделены N-граммы, вычислены их частоты. 
Дан контекст: `(0, 3)` и необходимое количество предложений: `3`. Необходимо исходя из этой информации сгенерировать текст.

Пусть идентификатор токена `<END>` равен 4, а первое сгенерированное предложения принимает следующий вид:
`(0, 3, 2, 1, 5, 126, 4)`. Для генерации следующего контекста мы будем использовать последние `n-1` токенов из предыдущего предложения, 
то есть токены `126` и `4`. Второе сгенерированное предложение: `(0, 7, 2, 4)`.

Пример сгенерированного текста:
`(0, 3, 2, 1, 5, 126, 4, 0, 7, 2, 4, 0, 1, 437, 3, 55, 4)`. Обратите внимание, 
что 4 - идентификатор `<END>` - встречается 3 раза: столько предложения и нужно было сгенерировать.


### Шаг 5. Использование алгоритма рассчёта максимального правдоподобия для генерации слов

Вероятность последовательности слов рассчитывается следующим образом:

![](https://latex.codecogs.com/gif.latex?P(x^{1}&space;...&space;x^{t})&space;=&space;P(x^{1})&space;\cdot&space;P(x^{2}|x^{1})&space;\cdot&space;...&space;\cdot&space;P(x^{t}|x^{t-1}&space;...&space;x^{1}))

Здесь мы говорим о том, что вероятность такой последовательности слов ![](https://latex.codecogs.com/gif.latex?P(x^{1}&space;...&space;x^{t})) 
является результатом перемножения вероятностей появления некоторого слова ![](https://latex.codecogs.com/gif.latex?P(x^{1})) после другой 
последовательности слов из `t-1` элементов до этого слова. 

Это [*алгоритм максимального правдоподобия*](https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1), который в данном случае можно описать так:

Вероятность появления слова `w` после последовательности из `n-1` слов рассчитывается следующим образом:
 * Количество раз, которое слово `w` встретилось после последовательности из `n-1` / количество раз, которое 
 последовательность `n-1` встретилась в корпусе.

У нас уже есть модель для генерирования текста, в которой используется относительно простой алгоритм. Мы можем сделать 
другой алгоритм генерации, но оставить тот же *интерфейс*, что и у предыдущей модели. Здесь мы можем прибегнуть к *наследованию*.

**Интерфейс**:
```python
class LikelihoodBasedTextGenerator(NGramTextGenerator):
  pass
```

Способ создания экземпляра класса остаётся таким же, как и у `NGramTextGenerator`.

#### Шаг 5.1. Использование алгоритма рассчёта максимального правдоподобия для генерации слов

Мы можем добавить метод для подсчёта вероятности появления слова в данном контексте.

Вероятность появления слова `w` после последовательности из `n-1` слов рассчитывается следующим образом:
 * Количество раз, которое слово `w` встретилось после последовательности из `n-1` / количество раз, которое 
 последовательность `n-1` встретилась в корпусе.

**Интерфейс** функции вычисления максимального правдоподобия:
```python
class LikelihoodBasedTextGenerator(NGramTextGenerator):
  ...
  def _calculate_maximum_likelihood(self, word: int, context: tuple) -> float:
      pass
```

> **Важно** Аргумент `context` - `n-1` до текущего слова `word`. 

> **Важно** В этой функции ОБЯЗАТЕЛЬНО использовать поле `n_gram_frequencies` экземпляра класса `NGramTrie`.

Если контекста нет в хранилище N-грамм, то возвращается `0.0`.

Если приходят некорректные значения, то поднимается ошибка `ValueError`.

**Пример**:

Есть модель с размером N-грамм равным 3, она уже обучена на некотором тексте, выделены N-граммы, вычислены их частоты. 
Дан контекст: `(0, 3)` и слово `2`. Необходимо вычислить вероятность появления данного слова в данном контексте.

Мы находим частотность N-граммы `(0, 3, 2)` - сколько раз данная N-грамма встречалась в тексте.
Находим количество N-грамм, которые начинаются с данного контекста - `(0, 3)`.

Пусть частотность N-граммы `(0, 3, 2)` равна 10, а количество N-грамм, начинающихся с `(0, 3)` равно 100.
Тогда вероятность появления слова `2` в данном контексте равняется отношению этих величин: `10/100`.

#### Шаг 5.2. Генерация следующего слова

Теперь необходимо переопределить генерацию следующего слова с использованием алгоритма максимального правдоподобия, 
описанного в предыдущем шаге.

Для генерации слова нам необходимо: 
 * Контекст (последние `n-1` слов);
 * Частотное распределение N-грамм в языке.

С использованием алгоритма максимального правдоподобия генерация слова будет выглядеть следующим образом:
1. Для каждого слова из имеющихся в словаре посчитать вероятность его
   появления после `n-1` слов - контекста;
2. Выбрать слово с наибольшей вероятностью.

**Интерфейс**:
```python
class LikelihoodBasedTextGenerator(NGramTextGenerator):
  ...
  def _generate_next_word(self, context: tuple) -> int:
      pass
```

Если приходят некорректные значения (или некорректный ввод `context`, или его неправильный размер), то поднимается ошибка `ValueError`.

Если данного контекса нет в словаре N-грамм, то возвращается наиболее частотное слово.

> **Важно** В этой функции ОБЯЗАТЕЛЬНО использовать поле `storage` экземпляра класса `WordStorage` и метод `_calculate_maximum_likelihood`.

**Пример**:

Есть модель с размером N-грамм равным 3, она уже обучена на некотором тексте, выделены N-граммы, вычислены их частоты. 
Дан контекст: `(0, 3)`. Необходимо по этому контексту предсказать следующее слово.

Вычисляем для каждого из слов в словаре его вероятность. 
Пример вычисленных вероятностей:
```
2: 0.124
3: 0.000001
1245: 0.7
...
```

Выбираем слово с наибольшей вероятностью - оно и будет предсказанным словом.

Логика методов `_generate_sentence` и `generate_text` остаётся без изменений. 

Примеры использования класса должны быть продемонстрированы в `start.py`.

### Шаг 6. Декодирование. Выполнение Шагов 1-6 соответствует 8 баллам

Предсказанный текст нам предстаёт в виде чисел, которые совсем не понятны человеку, следовательно, нам нужно этот закодированный текст декодировать.

**Интерфейс**:
```python
def decode_text(storage: WordStorage, encoded_text: tuple) -> tuple:
  pass
```

Если приходит некорректное значение, то поднимается ошибка `ValueError`.

>**Важно** Здесь должен использоваться тот же экземпляр `storage`, который был использован при кодировании текста.

Функция должна возвращать кортеж из сгенерированных предложений. 

**Пример**:
```python
text = ('He is afraid', 'She is not')
```

> **Важно** Первое слово в каждом предложении должно начинаться с большой буквы.

> **Важно** Специальные токен `<END>` не должен оказаться в результирующих предложениях.
> Его и нужно использовать для понимания, где предложение заканчивается. 

### Шаг 7. Использование модели back-off N-грамм для генерации текста

Может возникнуть ситуация, когда контекста и\или слова нет в хранилище N-грамм или в словаре соответственно. Это ведёт к тому,
что у нас нет вероятностного распределения и, соответственно, мы не можем предсказать следующее слово.

Одним из способов решения данной проблемы является использование [алгоритма back-off](https://en.wikipedia.org/wiki/Katz's_back-off_model) 
в языковой модели N-грамм.

Суть заключается в следующем:
 * Если не была найдена N-грамма размера `n` - то есть такого контекста для слова `w` - происходит переход к модели языка, 
которая была обучена с размером N-грамм = `n-1`. И уже с помощью данных этой модели происходит предсказание;
 * Если же не было найдено N-грамм с размером `n-1`, то происходит переход к модели с `n-2` - таким образом можно дойти до
 `n = 1`, то есть до уни-грамм.

Так как и в этой модели мы будем предсказывать слово-предложение-текст, что очень похоже на задачи `NGramTextGenerator` - 
только с несколько другими условиями, представляется возможным оставить тот же самый *интерфейс* и использовать *наследование*.

#### Шаг 7.1. Объявление back-off N-грамм модели предсказания текста

Создадим класс, который будет наследовать класс `NGramTextGenerator`:
```python
class BackOffGenerator(NGramTextGenerator):
    pass
```

Натренированные модели `NGramTrie` передаются при создании экзепляра класса следующим образом:
```python
back_off_generator = BackOffGenerator(word_storage=storage, n_gram_trie=trie1, trie2, trie3 ...)
```

> **Важно** Количество моделей `NGramTrie` может быть различно, поэтому передача в конструктор класса идёт через `*args`.

Все из переданных `NGramTrie` должны быть сохранены в поле класса `_n_gram_tries`.

#### Шаг 7.2. Генерация следующего слова

Для генерации слова нам необходимо: 
 * Контекст (последние `n-1` слов);
 * Распределение N-грамм в языке;
 * Дополнительные обученные языковые модели, основанные на N-граммах.

Алгоритм генерации слова очень похож на тот, что используется в базовой модели `NGramTextGenerator`, с той модификацией, 
что если мы не находим такого контекста, который нам был задан, мы переходим на модель с `n-1` длиной N-грамм. 
Таким образом можно перейти и на ступень `n-2` и так далее.

Если в каждой из моделей не было найдено подходящего контекста, происходит переход к уни-граммам, которые есть в каждой 
обученной модели `NGramTrie`. В качестве генерируемого слова выбирается самая частотная из них.

**Интерфейс**:
```python
class BackOffGenerator(NGramTextGenerator):
  ...
  def _generate_next_word(self, context: tuple) -> int:
      pass
```

Если приходят некорректные значения, то поднимается ошибка `ValueError`.

> **Важно** В этой функции ОБЯЗАТЕЛЬНО использовать поля `_word_storage` и `_n_gram_tries`.

**Пример**:

Есть модель с размером N-грамм равным 4, она уже обучена на некотором тексте, выделены N-граммы, вычислены их частоты. 
Также были переданы обученные модели с размерами N-грамм 2 и 3.  
Дан контекст: `(0, 3, 61)`. Необходимо по этому контексту предсказать следующее слово.

Пусть в хранилище модели с размером N-грамм равным 4 не нашлось таких N-грамм, которые бы начинались с `(0, 3, 61)` - контекста.
Тогда происходит переход на модель с размером N-грамм равным 3. Ищуются все N-грамм, которые начинаются с `(0, 3)` - нового контекста.

Представим, что таких N-грамм там несколько:
```
(0, 3, 2): 10,
(0, 3, 15): 2,
(0, 3, 999): 1,
...
``` 

Исходя из данных вероятностей следующее слово имеет идентификатор `2`, так как N-грамма `(0, 3, 2)` имеет наибольшую частоту 
встречания в тексте.

Логика методов `_generate_sentence` и `generate_text` остаётся без изменений. 

Примеры использования класса должны быть продемонстрированы в `start.py`.

### Шаг 8. Переиспользование модели. Дополнительное задание: даёт +1 балл к любому уровню

Процесс обучения модели - в нашем случае подсчёт частот N-грамм - может занимать большое количество времени, особенно на
больших корпусах. К тому же, хотелось бы использовать обученную модель через некоторое время или на другом устройстве.

Для этого необходимо реализовать возможность *сохранения* и *загрузки* модели. Эти способы необходимо разработать самостоятельно.
Про общую идею *сериализации* данных можно прочитать [здесь](https://ru.wikipedia.org/wiki/Сериализация).

> **Важно** Использование пакета `pickle` запрещено.

### Шаг 8.1. Сохранение модели 

Необходимо реализовать сохранение языковой модели в файловую систему.

**Интерфейс**:
```python
def save_model(model: NGramTextGenerator, path_to_saved_model: str):
    pass
``` 

Если приходят некорректные значения, то поднимается ошибка `ValueError`.

#### Шаг 8.2. Загрузка модели

Необходимо реализовать загрузку языковой модели из файла.

**Интерфейс**:
```python
def load_model(path_to_saved_model: str) -> NGramTextGenerator:
    pass
``` 

Если приходят некорректные значения, то поднимается ошибка `ValueError`.
